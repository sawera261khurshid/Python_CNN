# -*- coding: utf-8 -*-
"""fc_fp16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UUh16hvcLvVPqu31GvDFN7dxigNAyl87
"""

import numpy as np

class FC:
  # A standard fully-connected layer with softmax activation.

  def __init__(self, input_len, nodes, loss_scaling = 128.0):
    # We divide by input_len to reduce the variance of our initial values
    self.weights = np.random.randn(input_len, nodes).astype(np.float16) / input_len
    self.biases = np.zeros(nodes, dtype=np.float16)
    self.loss_scaling = loss_scaling
    np.save('linear_filters',self.weights)
    np.save('linear_biases' ,self.biases)


  def forward(self, input, loss = None):
    '''
    Performs a forward pass of the FC layer using the given input.
    Returns a 1d numpy array containing the respective values.
    - input can be any array with any dimensions.
    '''
    self.last_input_shape = input.shape
    input = input.flatten().astype(np.float16)
    self.last_input = input.astype(np.float16)

    input_len, nodes = self.weights.shape

    totals = np.dot(input, self.weights).astype(np.float32) + self.biases
    if loss is not None:
        scaled_loss = loss * self.loss_scaling
    self.last_totals = totals

    # loss = self.loss_function(totals, labels)
    # scaled_loss = loss*self.loss_scaling

    return totals.astype(np.float16), scaled_loss



  def backprop(self, d_L_d_t_scaled, learn_rate):
    '''
    Performs a backward pass of the softmax layer.
    Returns the loss gradient for this layer's inputs.
    - d_L_d_out is the loss gradient for this layer's outputs.
    - learn_rate is a float.
    '''
    # We know only 1 element of d_L_d_out will be nonzero
    # for i, gradient in enumerate(d_L_d_out):
    #   if gradient == 0:
    #     continue

    # Gradients of totals against weights/biases/input
    # d_L_d_t = d_L_d_t_scaled / self.loss_scaling
    d_t_d_w = self.last_input.astype(np.float16)
    d_t_d_b = 1
    d_t_d_inputs = self.weights.astype(np.float16)

    # Gradients of loss against weights/biases/input
    d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t_scaled[np.newaxis]
    d_L_d_b = d_L_d_t_scaled * d_t_d_b
    d_L_d_inputs = d_t_d_inputs @ d_L_d_t_scaled

    # Update weights / biases
    self.weights -= learn_rate * d_L_d_w.astype(np.float32)
    self.biases -= learn_rate * d_L_d_b.astype(np.float32)

    return d_L_d_inputs.reshape(self.last_input_shape).astype(np.float16)
