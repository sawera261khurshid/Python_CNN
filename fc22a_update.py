import numpy as np

class FC:
  # A standard fully-connected layer with softmax activation.

  def __init__(self, input_len, nodes, activation = None, dtype = np.float32):
    # We divide by input_len to reduce the variance of our initial values
    
    self.weights = np.random.randn(input_len, nodes).astype(dtype)  / input_len
    self.biases = np.zeros(nodes, dtype = dtype)
    self.activation = activation
    self.dtype = dtype
    np.save('linear_filters',self.weights)
    np.save('linear_biases' ,self.biases)
    
  def initialize_weights(self):
    self.weights = np.random.randn(*self.weights.shape).astype(self.dtype) / self.weights.shape[0]
    self.biases = np.zeros(self.biases.shape, dtype=self.dtype)
  def forward(self, input):
    '''
    Performs a forward pass of the FC layer using the given input.
    Returns a 1d numpy array containing the respective values.
    - input can be any array with any dimensions.
    '''
    self.last_input_shape = input.shape
    input = input.flatten().astype(self.dtype)
    self.last_input = input

    input_len, nodes = self.weights.shape

    totals = np.dot(input, self.weights) + self.biases
    self.last_totals = totals
    if self.activation is not None:
        return self.activation(totals)
    else:
        return totals


  
  

  def backprop(self, d_L_d_t, learn_rate):
    if self.activation is not None:
        d_L_d_t *= self.activation.gradient(self.last_totals)

    # Gradients of totals against weights/biases/input
    d_t_d_w = self.last_input
    d_t_d_b = 1
    d_t_d_inputs = self.weights.astype(self.dtype)

    # Gradients of loss against weights/biases/input
    d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]
    d_L_d_b = d_L_d_t * d_t_d_b
    d_L_d_inputs = d_t_d_inputs @ d_L_d_t

    # Update weights / biases
    self.weights -= learn_rate * d_L_d_w
    self.biases -= learn_rate * d_L_d_b

    return d_L_d_inputs.reshape(self.last_input_shape)













# # -*- coding: utf-8 -*-
# """fc22a_update.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1aSkAXIqSgqev5V5mczmtxJ-DBaNe9TEO
# """

# import numpy as np

# class FullyConnectedLayer:
#     def __init__(self, input_size, output_size, activation=None, dtype=np.float32):
#         self.input_size = input_size
#         self.output_size = output_size
#         self.activation = activation
#         self.dtype = dtype

#         # Initialize weights and biases
#         self.weights = np.random.randn(output_size, input_size).astype(dtype) / np.sqrt(input_size)
#         self.biases = np.zeros(output_size, dtype=dtype)

#     def initialize_weights(self):
#         self.weights = np.random.randn(self.output_size, self.input_size).astype(self.dtype) / np.sqrt(self.input_size)
#         self.biases = np.zeros(self.output_size, dtype=self.dtype)

#     def forward(self, input):
#         '''
#         Performs a forward pass of the FC layer using the given input.
#         Returns a 1D numpy array with dimensions (output_size).
#         - input is a 1D numpy array
#         '''
#         self.last_input = input
#         output = np.dot(self.weights, input) + self.biases
#         self.last_output = output
#         if self.activation is not None:
#             output = self.activation(output)
#         return output

#     def backprop(self, d_L_d_out, learn_rate):
#         d_L_d_input = np.dot(self.weights.T, d_L_d_out)
#         d_L_d_weights = np.outer(d_L_d_out, self.last_input)
#         d_L_d_biases = d_L_d_out

#         # Update weights and biases
#         self.weights -= learn_rate * d_L_d_weights
#         self.biases -= learn_rate * d_L_d_biases

#         return d_L_d_input

#     def get_weights(self):
#       return self.filters

#     def set_weights(self, weights):
#       self.weights = weights
